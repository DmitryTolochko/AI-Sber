# 1. –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏

- **`finetuning/`** ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–∞ –º–æ–¥–µ–ª–∏.  
  –í—Å—è –ª–æ–≥–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∞ –Ω–∞ —Ç—Ä–∏ —Ñ–∞–π–ª–∞:

  - **`finetuning.py`** ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è.  
    –ü—Ä–æ–≤–µ–¥—ë–Ω –Ω–µ–±–æ–ª—å—à–æ–π —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥: —á–∞—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏–π –≤—ã–Ω–µ—Å–µ–Ω–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã, –¥–æ–±–∞–≤–ª–µ–Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ `finetune_cfg.yaml`.
  - **`preprocessor.py`** ‚Äî —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –∏ –æ—á–∏—Å—Ç–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º.
  - **`utils.py`** ‚Äî –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLTK, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, —É—Ç–∏–ª–∏—Ç—ã –∏ –ø—Ä.).

- **`finetuning/datasets/`** ‚Äî –ø–∞–ø–∫–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –∏ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.

- **`models/`** ‚Äî –≤—Å–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏:
  - **`/core-model`** ‚Äî –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ –ø—Ä–æ–µ–∫—Ç.
  - **`/ft`** ‚Äî –º–æ–¥–µ–ª–∏, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –ø–æ—Å–ª–µ —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–∞.
    > ‚ö†Ô∏è –≠—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ `.gitignore`, —á—Ç–æ–±—ã –Ω–µ –ø—É—à–∏—Ç—å –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —Ç—è–∂—ë–ª—ã–µ —Ñ–∞–π–ª—ã (–Ω–µ—Å–∫–æ–ª—å–∫–æ –ì–ë).
  - **`/hf`** ‚Äî –∫—ç—à –º–æ–¥–µ–ª–µ–π —Å Hugging Face.
    > ‚ö†Ô∏è –¢–∞–∫–∂–µ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è –≤ `.gitignore`.

---

# 2. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ —Ñ–∞–π–ª–µ: **`finetuning/fintune_cfg.yaml`**

–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è **`finetune_cfg.yaml`**, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–æ–∑–¥–∞—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∞–π–ª–∞ **`finetune_cfg.example.yaml`**, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ —Å–æ–∑–¥–∞—Ç—å, —Ç–æ –±—É–¥–µ—Ç –∏–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è **example** - —Ñ–∞–π–ª.


## ‚öôÔ∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ `finetune_cfg.yaml`

–§–∞–π–ª **`finetuning/finetune_cfg.yaml`** –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤—Å–µ–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LoRA.  
–û–Ω —Ä–∞–∑–¥–µ–ª—ë–Ω –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤: `paths`, `training`, `tokenizer` –∏ `lora`.  
–ù–∏–∂–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–æ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—è.

---

### üìÇ `paths`

–ü—É—Ç–∏ –∫ –æ—Å–Ω–æ–≤–Ω—ã–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º –∏ —Ñ–∞–π–ª–∞–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–º –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏.

| –ö–ª—é—á | –û–ø–∏—Å–∞–Ω–∏–µ | –ü—Ä–∏–º–µ—Ä |
|------|-----------|--------|
| `MODEL_ID` | –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ Hugging Face (–∏–ª–∏ –ø—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏) | `facebook/mbart-large-50-many-to-many-mmt` |
| `DATA_PATH` | –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ `.json`, –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–º—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è | `finetuning/datasets/augmented_all.json` |
| `CACHE_DIR` | –ü–∞–ø–∫–∞ –¥–ª—è –∫—ç—à–∞ –º–æ–¥–µ–ª–µ–π Hugging Face (–≥–¥–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –≤–µ—Å–∞ –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä—ã) | `models/hf/facebook/mbart-large-50-many-to-many-mmt` |
| `OUTPUT_DIR` | –ü–∞–ø–∫–∞, –≤ –∫–æ—Ç–æ—Ä—É—é —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è –∏ —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–∞ –º–æ–¥–µ–ª–∏ | `models/ft/nanai_lora-facebook/mbart-large-50-many-to-many-mmt` |

---

### üß† `training`

–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è.

| –ö–ª—é—á | –û–ø–∏—Å–∞–Ω–∏–µ | –ü—Ä–∏–º–µ—Ä |
|------|-----------|--------|
| `BATCH_SIZE` | –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∑–∞ –æ–¥–Ω—É –∏—Ç–µ—Ä–∞—Ü–∏—é) | `8` |
| `GRADIENT_ACCUMULATION_STEPS` | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –∞–∫–∫—É–º—É–ª—è—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–µ—Ä–µ–¥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –≤–µ—Å–æ–≤ (–¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM) | `4` |
| `EPOCHS` | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è | `10` |
| `LEARNING_RATE` | –ù–∞—á–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è | `5e-4` |
| `EVAL_STEPS` | –ß–∞—Å—Ç–æ—Ç–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ (—á–µ—Ä–µ–∑ —Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤ –ø—Ä–æ–≤–æ–¥–∏—Ç—å –æ—Ü–µ–Ω–∫—É) | `1000` |
| `SAVE_STEPS` | –ß–∞—Å—Ç–æ—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ | `1000` |
| `LOGGING_STEPS` | –ß–∞—Å—Ç–æ—Ç–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ–±—É—á–µ–Ω–∏–∏ | `100` |
| `SAVE_TOTAL_LIMIT` | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ (—Å—Ç–∞—Ä—ã–µ —É–¥–∞–ª—è—é—Ç—Å—è) | `2` |
| `GENERATION_MAX_LENGTH` | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ | `128` |
| `GENERATION_NUM_BEAMS` | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª—É—á–µ–π (beam search) –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ | `1` |
| `LR_SCHEDULER_TYPE` | –¢–∏–ø –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è (`linear`, `cosine`, `constant` –∏ –¥—Ä.) | `"cosine"` |
| `WARMUP_STEPS` | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –ø—Ä–æ–≥—Ä–µ–≤–∞ –ø–µ—Ä–µ–¥ –æ—Å–Ω–æ–≤–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º (LR –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è) | `500` |

---

### ‚úÇÔ∏è `tokenizer`

–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –¥–∞–Ω–Ω—ã—Ö.

| –ö–ª—é—á | –û–ø–∏—Å–∞–Ω–∏–µ | –ü—Ä–∏–º–µ—Ä |
|------|-----------|--------|
| `SRC_LANG` | –ö–æ–¥ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —è–∑—ã–∫–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –º–æ–¥–µ–ª–µ–π —Ç–∏–ø–∞ mBART) | `"en_XX"` |
| `TGT_LANG` | –ö–æ–¥ —Ü–µ–ª–µ–≤–æ–≥–æ —è–∑—ã–∫–∞ (–¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –∏–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞) | `"ru_RU"` |
| `MAX_LENGTH` | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ | `128` |
| `LEMMATIZE` | –§–ª–∞–≥ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫ —Ç–µ–∫—Å—Ç–∞–º –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ (`true`/`false`) | `false` |

---

### üîß `lora`

–ù–∞—Å—Ç—Ä–æ–π–∫–∏ **LoRA (Low-Rank Adaptation)** –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–∞ –º–æ–¥–µ–ª–∏.

| –ö–ª—é—á | –û–ø–∏—Å–∞–Ω–∏–µ | –ü—Ä–∏–º–µ—Ä |
|------|-----------|--------|
| `R` | –†–∞–Ω–≥ –º–∞—Ç—Ä–∏—Ü—ã –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ LoRA (–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏) | `8` |
| `LORA_ALPHA` | –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è LoRA | `32` |
| `TARGET_MODULES` | –°–ø–∏—Å–æ–∫ –º–æ–¥—É–ª–µ–π –º–æ–¥–µ–ª–∏, –∫ –∫–æ—Ç–æ—Ä—ã–º –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è LoRA (–Ω–∞–ø—Ä–∏–º–µ—Ä, `q_proj`, `v_proj`) | `["q_proj", "v_proj"]` |
| `LORA_DROPOUT` | –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥—Ä–æ–ø–∞—É—Ç–∞ –≤–Ω—É—Ç—Ä–∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–ª–æ—ë–≤ LoRA | `0.1` |
| `BIAS` | –ü–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏–µ–º —Å–º–µ—â–µ–Ω–∏–π (`"none"`, `"all"`, `"lora_only"`) | `"none"` |

---

### üß© –ü—Ä–∏–º–µ—Ä —Ñ–∞–π–ª–∞ `finetune_cfg.yaml`

```yaml
paths:
  MODEL_ID: facebook/mbart-large-50-many-to-many-mmt
  DATA_PATH: finetuning/datasets/augmented_all.json
  CACHE_DIR: models/hf/facebook/mbart-large-50-many-to-many-mmt
  OUTPUT_DIR: models/ft/nanai_lora-facebook/mbart-large-50-many-to-many-mmt

training:
  BATCH_SIZE: 8
  GRADIENT_ACCUMULATION_STEPS: 4
  EPOCHS: 10
  LEARNING_RATE: 5e-4
  EVAL_STEPS: 1000
  SAVE_STEPS: 1000
  LOGGING_STEPS: 100
  SAVE_TOTAL_LIMIT: 2
  GENERATION_MAX_LENGTH: 128
  GENERATION_NUM_BEAMS: 1
  LR_SCHEDULER_TYPE: "cosine"
  WARMUP_STEPS: 500

tokenizer:
  SRC_LANG: "en_XX"
  TGT_LANG: "ru_RU"
  MAX_LENGTH: 128
  LEMMATIZE: false

lora:
  R: 8
  LORA_ALPHA: 32
  TARGET_MODULES: ["q_proj", "v_proj"]
  LORA_DROPOUT: 0.1
  BIAS: "none"
